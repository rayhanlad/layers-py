Educational project to have a better understanding of how the deep learning layers work and are implemented. 
Nowadays, it is very easy to build a neural network, we just have to use keras or pytorch libraries and stack layers; run the training processus and wait; if the results are not "satisfaying" we redo the experiments until we have "satisfying" results.
Have a better understanding of how the layers and algorithms works might be useful to avoid unnecessary steps in the iterative process of finding "satisfying" results.
That's why I have decided to re-implement all these algorithms and beautiful objects. 


In construction...

To do:
 1. CNN Layers (backward).
 2. RNN Layers (forward and backward pass)
 3. Add SoftMax loss (and other fancy losses...)
 4. Complete the activations (Add LeakyReLU, and others...)
 5. Find a clever implementation of the Optimizer class.
 6. Documentation
 7. Implement Pooling layer
 8. Implement Dropout layer.